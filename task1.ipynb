{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Олимпиада НТИ - как выйграть за 30 строк"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pp.userapi.com/c638719/v638719005/2f6da/mFHY83I9u2k.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Машинное обучение и анализ данных - задача №1\n",
    "\n",
    "\n",
    "Описание предметной области. Выявление информации из текста - одна из самых сложных и интересных задач машинного обучения. Речь идет не о разборе предложения или структуре текстов, а о таких вопросах как определения авторства или стиля. Говоря о машинном обучении неизбежно встает вопрос о том, как программе обрабатывать и находить новую информацию в огромных объемах данных, которые даже человек не может осмыслить. Например, нет такого в мире специалиста, который бы мог точно определить авторство любой заметки из 200 авторов начала 20го века. Возможность обучения без учителя, когда вручную отмечаются правильные ответы лишь на небольшом количестве данных - одна из самых актуальных задач машинного обучения. Ведь как только алгоритмы смогут эффективно обучаться на больших объемах без предварительной ручной разметки создание сильного искусственного интеллекта будет сильно ближе.\n",
    "\n",
    "Описание актуальной задачи. В качестве данных предлагается анализировать личные дневники 19го и начала 20го века. Настоящее авторство текстов - популярный вопрос среди искусствоведов, дискуссии о котором не утихают десятилетиями. Благодаря аналитике больших данных можно не просто сказать к чьему перу относится запись, но и выявить под чей стиль она подходит и выявить важные факты, передав искусствоведам эффективный инструмент анализа. Однако, возможен случай когда у набора записей не осталось сведения об авторах, и список авторов неограничен. Чтобы разобрать эти записи можно применять кластеризацию, и находить схожие заметки. При этом, в обучающей выборке лишь небольшое количество размеченных записей (около 2000), и большое (десятки тысяч) неразмеченных данных. Необходимо реализовать обучение с частичным привлечением учителя (semi-supervised learning).\n",
    "\n",
    "Всего на сайте около 200 авторов, и 100 000 записей дневников.\n",
    "\n",
    "В качестве обучающей выборки представлено 100 000 записей, из которых 2.000 будет отмечено авторство."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача содержит 3 датасета:\n",
    "\n",
    "Первые два даются участникам, а на основе третьего генерируется score.\n",
    "\n",
    "Давайте скачаем эти датасеты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://tvorog.me/task1_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://tvorog.me/task1_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет с ответами.\n",
    "Его мы используем только для проверки ответов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://tvorog.me/task1_answers.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Посмотрим на данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pandas](http://pandas.pydata.org/) это бог табличной PyData. Неплохие примеры его использования можно найти [тут](http://pandas.pydata.org/pandas-docs/stable/tutorials.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas позволяет очень просто грузить csv\n",
    "test = pd.DataFrame.from_csv(\"task1_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# .head() показывает первые 5 элементов таблицы\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame.from_csv(\"task1_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К элементам таблицы можно обращаться двумя способами.\n",
    "\n",
    "1. train['text']\n",
    "2. train.text\n",
    "\n",
    "Они возвращают запрашиваемый столбец. Единственная загвоздка заключается в index колонке. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.note_id\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "AttributeError                            Traceback (most recent call last)\n",
    "<ipython-input-26-ed64215d46cd> in <module>()\n",
    "----> 1 train.note_id\n",
    "\n",
    "/home/tvorog/.anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __getattr__(self, name)\n",
    "   2742             if name in self._info_axis:\n",
    "   2743                 return self[name]\n",
    "-> 2744             return object.__getattribute__(self, name)\n",
    "   2745 \n",
    "   2746     def __setattr__(self, name, value):\n",
    "\n",
    "AttributeError: 'DataFrame' object has no attribute 'note_id'```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counter считает количество объектов в итераторе и возвращает tuple (item: count)\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# .most_common() сортирует по count\n",
    "Counter(train.person_id).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Counter(test.person_id).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Организаторы во всём виноваты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images5.alphacoders.com/633/633216.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании было сказанно о том, что размеченных авторов примерно 2 тысячи и поэтому нужно использовать semi. Так ли это?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# В pandas можно выбирать элементы с помощью банальных выражений, это все очень похоже на sql\n",
    "len(test[test.person_id != -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не смотря на то, что организаторы во всём виноваты, мы с вами хотим выйграть олимпиаду НТИ. Сейчас я покажу на сколько просто это сделать. Наш baseline будет состоять из трёх частей. \n",
    "\n",
    "1. Чистка текста\n",
    "2. Векторизация текста\n",
    "3. Создание и тренировка модели\n",
    "\n",
    "Использовать мы будем только test датасет, потому что 40к текстов - это вполне нормальная выборка для обучении модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### №1 Чистка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы преобразовать все слова в начальную форму достаточно скачать библиотеку [pymorphy2](https://pymorphy2.readthedocs.io/en/latest/). Она распарсит нужные вам слова и приведет их в начальную форму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pip\n",
    "pip.main(['install','pymorphy2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Для того, чтобы исключить все лишние знаки будем использовать regexp\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Уникальных слов, вангую, очень много. Давайте кэшировать.\n",
    "global_morph = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(text: str) -> str:\n",
    "    '''Функция, которая принимает строку и возвращает строку. Чистит текст и сводит к начальной форма'''\n",
    "    global global_morph\n",
    "    \n",
    "    # Приведем все в нижний регистр\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Заменим все бяки *не буквы* на пустоту\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Возьмём все слова\n",
    "    words = re.findall(r'\\w+', text)\n",
    "    \n",
    "    tmp = \"\"\n",
    "    \n",
    "    # Для каждого слова\n",
    "    for word in words:\n",
    "        # Если оно закешированно\n",
    "        if word in global_morph.keys():\n",
    "            # Возьмём его\n",
    "            tmp += global_morph[word].normal_form\n",
    "            tmp += \" \"\n",
    "        # Иначе\n",
    "        else:\n",
    "            # Закешируем\n",
    "            global_morph[word] = morph.parse(word)[0]\n",
    "            \n",
    "            # Возьмём его\n",
    "            tmp += global_morph[word].normal_form\n",
    "            tmp += \" \"\n",
    "\n",
    "    return tmp[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas позволяет применить ко всей колонке функцию.\n",
    "test.text = test.text.apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## №2 Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всемогучий [sklearn](http://scikit-learn.org/stable/). Представьте, если бы была библиотека, которая за вас может выйграть олимпиаду... Так вот, она есть. Именуется sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://s9.pikabu.ru/post_img/2016/12/12/8/og_og_1481547098267285367.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного wikipedia:\n",
    "\n",
    "```\n",
    "TF-IDF (от англ. TF — term frequency, IDF — inverse document frequency) — статистическая мера, используемая для оценки важности слова в контексте документа, являющегося частью коллекции документов или корпуса. Вес некоторого слова пропорционален количеству употребления этого слова в документе, и обратно пропорционален частоте употребления слова в других документах коллекции.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала нужно взять все слова в текстах и посчитать сколько раз они там встречаются *для того, чтобы это было возможно мы и приводили всё в начальную форму*. К счастью это можно сделать в одну строчку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer.fit(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = test[test.person_id != -1]\n",
    "data_predict =  test[test.person_id == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = vectorizer.transform(list(data_train['text']))\n",
    "y = list(data_train['person_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=170, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_answer = vectorizer.transform(list(data_predict['text']))\n",
    "answer = model.predict(X_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
