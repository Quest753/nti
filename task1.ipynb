{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Олимпиада НТИ - как выиграть за 30 строк"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pp.userapi.com/c638719/v638719005/2f6da/mFHY83I9u2k.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Машинное обучение и анализ данных - задача №1\n",
    "\n",
    "\n",
    "Описание предметной области. Выявление информации из текста - одна из самых сложных и интересных задач машинного обучения. Речь идет не о разборе предложения или структуре текстов, а о таких вопросах как определения авторства или стиля. Говоря о машинном обучении неизбежно встает вопрос о том, как программе обрабатывать и находить новую информацию в огромных объемах данных, которые даже человек не может осмыслить. Например, нет такого в мире специалиста, который бы мог точно определить авторство любой заметки из 200 авторов начала 20го века. Возможность обучения без учителя, когда вручную отмечаются правильные ответы лишь на небольшом количестве данных - одна из самых актуальных задач машинного обучения. Ведь как только алгоритмы смогут эффективно обучаться на больших объемах без предварительной ручной разметки создание сильного искусственного интеллекта будет сильно ближе.\n",
    "\n",
    "Описание актуальной задачи. В качестве данных предлагается анализировать личные дневники 19го и начала 20го века. Настоящее авторство текстов - популярный вопрос среди искусствоведов, дискуссии о котором не утихают десятилетиями. Благодаря аналитике больших данных можно не просто сказать к чьему перу относится запись, но и выявить под чей стиль она подходит и выявить важные факты, передав искусствоведам эффективный инструмент анализа. Однако, возможен случай когда у набора записей не осталось сведения об авторах, и список авторов неограничен. Чтобы разобрать эти записи можно применять кластеризацию, и находить схожие заметки. При этом, в обучающей выборке лишь небольшое количество размеченных записей (около 2000), и большое (десятки тысяч) неразмеченных данных. Необходимо реализовать обучение с частичным привлечением учителя (semi-supervised learning).\n",
    "\n",
    "Всего на сайте около 200 авторов, и 100 000 записей дневников.\n",
    "\n",
    "В качестве обучающей выборки представлено 100 000 записей, из которых 2.000 будет отмечено авторство."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача содержит 3 датасета:\n",
    "\n",
    "Первые два даются участникам, а на основе третьего генерируется score.\n",
    "\n",
    "Давайте скачаем эти датасеты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://tvorog.me/task1_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://tvorog.me/task1_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет с ответами.\n",
    "Его мы используем только для проверки ответов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://tvorog.me/task1_answers.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Посмотрим на данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pandas](http://pandas.pydata.org/) это бог табличной PyData. Неплохие примеры его использования можно найти [тут](http://pandas.pydata.org/pandas-docs/stable/tutorials.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas позволяет очень просто грузить csv\n",
    "test = pd.DataFrame.from_csv(\"task1_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# .head() показывает первые 5 элементов таблицы\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame.from_csv(\"task1_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К элементам таблицы можно обращаться двумя способами.\n",
    "\n",
    "1. train['text']\n",
    "2. train.text\n",
    "\n",
    "Они возвращают запрашиваемый столбец. Единственная загвоздка заключается в index колонке. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.note_id\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "AttributeError                            Traceback (most recent call last)\n",
    "<ipython-input-26-ed64215d46cd> in <module>()\n",
    "----> 1 train.note_id\n",
    "\n",
    "/home/tvorog/.anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __getattr__(self, name)\n",
    "   2742             if name in self._info_axis:\n",
    "   2743                 return self[name]\n",
    "-> 2744             return object.__getattribute__(self, name)\n",
    "   2745 \n",
    "   2746     def __setattr__(self, name, value):\n",
    "\n",
    "AttributeError: 'DataFrame' object has no attribute 'note_id'```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counter считает количество объектов в итераторе и возвращает tuple (item: count)\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# .most_common() сортирует по count\n",
    "Counter(train.person_id).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Counter(test.person_id).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Организаторы во всём виноваты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images5.alphacoders.com/633/633216.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании было сказанно о том, что размеченных авторов примерно 2 тысячи и поэтому нужно использовать semi. Так ли это?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# В pandas можно выбирать элементы с помощью банальных выражений, это все очень похоже на sql\n",
    "len(test[test.person_id != -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не смотря на то, что организаторы во всём виноваты, мы с вами хотим выйграть олимпиаду НТИ. Сейчас я покажу на сколько просто это сделать. Наш baseline будет состоять из трёх частей. \n",
    "\n",
    "1. Чистка текста\n",
    "2. Векторизация текста\n",
    "3. Создание и тренировка модели\n",
    "\n",
    "Использовать мы будем только test датасет, потому что 40к текстов - это вполне нормальная выборка для обучении модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### №1 Чистка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы преобразовать все слова в начальную форму достаточно скачать библиотеку [pymorphy2](https://pymorphy2.readthedocs.io/en/latest/). Она распарсит нужные вам слова и приведет их в начальную форму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pip\n",
    "pip.main(['install','pymorphy2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Для того, чтобы исключить все лишние знаки будем использовать regexp\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Уникальных слов, вангую, очень много. Давайте кэшировать.\n",
    "global_morph = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(text: str) -> str:\n",
    "    '''Функция, которая принимает строку и возвращает строку. Чистит текст и сводит к начальной форма'''\n",
    "    global global_morph\n",
    "    \n",
    "    # Приведем все в нижний регистр\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Заменим все бяки *не буквы* на пустоту\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Возьмём все слова\n",
    "    words = re.findall(r'\\w+', text)\n",
    "    \n",
    "    tmp = \"\"\n",
    "    \n",
    "    # Для каждого слова\n",
    "    for word in words:\n",
    "        # Если оно закешированно\n",
    "        if word in global_morph.keys():\n",
    "            # Возьмём его\n",
    "            tmp += global_morph[word].normal_form\n",
    "            tmp += \" \"\n",
    "        # Иначе\n",
    "        else:\n",
    "            # Закешируем\n",
    "            global_morph[word] = morph.parse(word)[0]\n",
    "            \n",
    "            # Возьмём его\n",
    "            tmp += global_morph[word].normal_form\n",
    "            tmp += \" \"\n",
    "\n",
    "    return tmp[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas позволяет применить ко всей колонке функцию.\n",
    "test.text = test.text.apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## №2 Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всемогучий [sklearn](http://scikit-learn.org/stable/). Представьте, если бы была библиотека, которая за вас может выйграть олимпиаду... Так вот, она есть. Именуется sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://s9.pikabu.ru/post_img/2016/12/12/8/og_og_1481547098267285367.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного wikipedia:\n",
    "\n",
    "```\n",
    "TF-IDF (от англ. TF — term frequency, IDF — inverse document frequency) — статистическая мера, используемая для оценки важности слова в контексте документа, являющегося частью коллекции документов или корпуса. Вес некоторого слова пропорционален количеству употребления этого слова в документе, и обратно пропорционален частоте употребления слова в других документах коллекции.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала нужно взять все слова в текстах и посчитать сколько раз они там встречаются *для того, чтобы это было возможно мы и приводили всё в начальную форму*. К счастью это можно сделать в одну строчку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer.fit(test.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше разобьём датасет на 2 части. В одной части соберём все записи, которые размечены на авторов. Во вротой части будут записи для которых мы должны предсказать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = test[test.person_id != -1]\n",
    "data_predict =  test[test.person_id == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы обучить модель нам нужно разбить всё на две части. Возьмём все тексты и положем их в X, возьмём всех авторов и положем их в y. Но поскольку наш `vectorizer` уже знает все слова в текстах, он может нам дать вектор для каждого текста, состоящий из TF-IDF значений. Давайте мы попросим его это сделать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = vectorizer.transform(list(data_train['text']))\n",
    "y = list(data_train['person_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## №3 Создание и тренировка модели\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку у нас много авторов - это задача регрессии. А если есть задача регрессии - то есть и модель, которая такую задачу решает. И, как не странно, называется она ```LogisticRegression```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ```LogisticRegression``` на результат влияет только одна константа: ```C```. Для улучшения обобщающей способности получающейся модели, то есть уменьшения эффекта переобучения существует регуляризация. ```C``` - это обратная степень регуляризации. Чем меньще ```C``` тем меньше переобучается модель, но тем сложнее обучить модель. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_jobs - кол-во процессеров, которое мы используем\n",
    "model = LogisticRegression(C=170, n_jobs=-1) # создадим модель с C = 170 *белка подсказала*.\n",
    "\n",
    "# ВАЖНО! Модель со 170 может обучаться очень долго *20 минут*, так что лучше поставить поменьше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь наша модель *обучена*. Это такое великолепное состояние, когда она может нам что-то дать. Но для того, чтобы она нам что-то дала, в неё нужно что-то положить!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_answer = vectorizer.transform(list(data_predict['text'])) # векторезуем текст\n",
    "answer = model.predict(X_answer) # и предсказываем по нему"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Done!\n",
    "#### А что теперь?\n",
    "\n",
    "Давайте поймём на сколько хороша наша модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Как я уже писал ранее, все ответы лежат в этом *забавном* csv.\n",
    "answers = pd.DataFrame.from_csv('task1_answers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека, которая за вас решает олимпиаду может вам сказать на сколько вы классные. В ```sklearn``` есть реализация ```accuracy_score```. ```accuracy_score``` - это процент того, сколько раз вы угадали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#сложно\n",
    "\n",
    "Дело в том, что нас просили предсказать для каждого текста в ```task1_test.csv``` автора. А это значит, что авторы, которые уже размечены тоже учитываются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте сопоставим индексы и ответы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_index_to_answer = {x:y for x,y in zip(data_train.index, data_train['person_id'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_index_to_answer = {x:y for x,y in zip(data_predict.index, answer)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь сгенерируем полный ответ на задачу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_answer = []\n",
    "\n",
    "for index in answers.index:\n",
    "    if index in train_index_to_answer.keys():\n",
    "        full_answer.append(train_index_to_answer[index])\n",
    "    else:\n",
    "        full_answer.append(test_index_to_answer[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ура. \n",
    "\n",
    "Посмотрим на ```score```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.setwalls.ru/pic/201305/1680x1050/setwalls.ru-43884.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(answers.person_id, full_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Что делать?\n",
    "\n",
    "1. Можно улучшать текущую модель с помощью sklearn.\n",
    "2. Можно улучшить обработку данных.\n",
    "3. Можно сделать настоящий ```semi-supervised learning```. Я сделаю отдельную тетрадку для этого. Вы можете помочь мне и сделать Pull Request в [мой репозиторий](https://github.com/xenx/nti-bigdata2017).\n",
    "4. Можно попробовать свои идеи для этой задачи, например, реализовать [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) или [SGDClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html), а лучше сделать [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) из этого."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
